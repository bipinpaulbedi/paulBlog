{"data":{"site":{"siteMetadata":{"title":"paul blog","description":"thoughts, stories & ideas.","author":"Bipin Paul Bedi","siteUrl":"https://www.bipinpaulbedi.com"}},"markdownRemark":{"id":"83fca52e-f506-543b-bb37-b7d9695f772c","excerpt":"A concurrent program has multiple logical threads of control. These threads may or may not run in parallel. A parallel program potentially…","html":"<p>A concurrent program has multiple logical threads of control. These threads may or may not run in parallel. A parallel program potentially runs more quickly than a sequential program by executing different parts of the computation simultaneously (in parallel). It may or may not have more than one logical thread of control.</p>\n<p>An alternative way of thinking about this is that concurrency is an aspect of the problem domain—your program needs to handle multiple simultaneous (or near-simultaneous) events. Parallelism, by contrast, is an aspect of the solution domain—you want to make your program faster by processing different portions of the problem in parallel</p>\n<p>Thus by representing an algorithm using asymptotic notation, it is much easier, faster and standard methodology to analyze and compare algorithms. For this post, we will restrict our discussion to time complexities {since the tremendous technological advancement has led to the cost of storage (persistent or ephemeral) as negligible}. In any problem domain, for a given algorithm f, with input size n we calculate some resultant runtime f(n). This results in a graph where the Y-axis is the runtime, X-axis is the input size, and plot points are the resultants of the amount of time for a given input size. We would mostly measure the worst-case scenario for any algorithm to compare different algorithms against the standard set of facts and dimentions.  </p>\n<p>Before analysing the algorithms, we shall establish certain common complexity classes in which an algorithm can be classified i.e. g(n) viz. K (or constant), log n, n, n*log n, n^2, n^3…, 2^n, 3^n…n^n  </p>\n<p><strong>Types of Asymptotic Notation</strong>  </p>\n<p><strong>Big-O</strong><br>\nBig-O, commonly written as O, is an Asymptotic Notation for the worst case, or ceiling of growth for a given function. It provides us with an asymptotic upper bound for the growth rate of the runtime of an algorithm.<br>\nFor e.g. f(n) is your algorithm runtime, and g(n) is an arbitrary time complexity you are trying to relate to your algorithm. f(n) is O(g(n)), if for some real constants c (c > 0) and n0, f(n) &#x3C;= c g(n) for every input size n (n > n0)  </p>\n<p><code class=\"language-text\">f(n) = O(g(n) For K and N0</code><br>\n<code class=\"language-text\">if f(n) &lt;= k * g(n) where n&gt;=n0</code><br>\n<code class=\"language-text\">e.g.</code><br>\n<code class=\"language-text\">f(n) = 2n^2 + 3n + 1</code>\n<code class=\"language-text\">since 2n^2 + 3n^2 + n2 = 6n^2</code><br>\n<code class=\"language-text\">f(n) = 2n^2 + 3n + 1 &lt;= 6n^2 for n &gt;= ?</code><br>\n<code class=\"language-text\">f(n) &lt;= k * g(n)</code><br>\n<code class=\"language-text\">i.e. 6 * n^2</code><br>\n<code class=\"language-text\">Thus f(n) = O(n^2)</code></p>\n<p><strong>Big-Omega</strong><br>\nBig-Omega, commonly written as Ω, is an Asymptotic Notation for the best case, or a floor growth rate for a given function. It provides us with an asymptotic lower bound for the growth rate of the runtime of an algorithm.<br>\nf(n) is Ω(g(n)), if for some real constants c (c > 0) and n0 (n0 > 0), f(n) is >= c g(n) for every input size n (n > n0).  </p>\n<p><code class=\"language-text\">f(n) = BIG-OMEGA(g(n) For K and N0</code><br>\n<code class=\"language-text\">if f(n) &gt;= k * g(n) where n&gt;=n0</code><br>\n<code class=\"language-text\">e.g.</code><br>\n<code class=\"language-text\">f(n) = 2n^2 + 3n + 1</code>\n<code class=\"language-text\">f(n) = 2n^2 + 3n + 1 &gt;= n^2 for n &gt;= ?</code><br>\n<code class=\"language-text\">f(n) &lt;= k * g(n)</code><br>\n<code class=\"language-text\">i.e. 1 * n^2</code><br>\n<code class=\"language-text\">or k * g(n)</code><br>\n<code class=\"language-text\">Thus f(n) = BIG-OMEGA(n^2)</code></p>\n<p><strong>Theta</strong><br>\nTheta, commonly written as Θ, is an Asymptotic Notation to denote the asymptotically tight bound on the growth rate of the runtime of an algorithm.<br>\ni.e. if O(g(n)) = Ω(g(n))<br>\nThen<br>\nf(n) = Θ(g(n))</p>\n<p><code class=\"language-text\">If f(n) = O(n2)</code><br>\n<code class=\"language-text\">and f(n) = BIG-OMEGA(n^2)</code><br>\n<code class=\"language-text\">also</code><br>\n<code class=\"language-text\">f(n) = O(g(n)) and f(n) = BIG-OMEGA(g(n))</code>\n<code class=\"language-text\">Then f(n) = THETA(g(n))</code><br>\n<code class=\"language-text\">Thus f(n) = THETA(n^2)</code></p>\n<p>Note\nThe asymptotic growth rates provided by big-O and big-omega notation may or may not be asymptotically tight. Thus we use small-o and small-omega notation to denote bounds that are not asymptotically tight.</p>\n<p>The main difference is that in f(n) = O(g(n)), the bound f(n) &#x3C;= g(n) holds for <strong>some</strong> constant c > 0, but in f(n) = o(g(n)), the bound f(n) &#x3C; c g(n) holds for <strong>all</strong> constants c > 0.\nSimilarly\nf(n) = Ω(g(n)), the bound f(n) >= g(n) holds for <strong>some</strong> constant c > 0, but in f(n) = ω(g(n)), the bound f(n) > c g(n) holds for <strong>all</strong> constants c > 0.</p>\n<p><code class=\"language-text\">Calculating for n!</code><br>\n<code class=\"language-text\">if f(n) = n!</code><br>\n<code class=\"language-text\">f(n) = n * (n-1) * (n-2)... 2 * 1</code>\n<code class=\"language-text\">For upper bound = n * n * n * n * n * n * n</code><br>\n<code class=\"language-text\">i.e. f(n) = n! &lt;= n^n for n&gt;=?</code><br>\n<code class=\"language-text\">f(n) = O(n^n)</code>\n<code class=\"language-text\">For lower bound = 1 * 1 * 1 * 1 * 1...1</code>\n<code class=\"language-text\">= k</code>\n<code class=\"language-text\">Thus f(n) =  BIG-OMEGA(1) or BIG-OMEGA(K)</code><br>\n<code class=\"language-text\">since O and BIG-OMEGA for n! is not equal it does not have a tight bound</code> </p>","fields":{"slug":"/2019-01-04-elixir-concurrency-models/"},"frontmatter":{"title":"phoenix/elixir - concurrency actor model with 'let it fail' philosophy","date":"January 04, 2019","tags":["elixir","phoenix","actor-model","article"],"subTitle":"concurrent or parallel processing models","categories":"technology"}}},"pageContext":{"slug":"/2019-01-04-elixir-concurrency-models/","previous":{"fields":{"slug":"/2018-10-20-asymtotic-notations/"},"frontmatter":{"title":"ELI5 - Asymptotic computational complexity simplified","tags":["biasing","algorithm","algorithm-design","article"],"categories":"algorithm-design"}},"next":null}}